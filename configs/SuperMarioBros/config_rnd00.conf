[DEFAULT]
TrainMethod = RND
representationLearningMethod = None


EnvType = mario
EnvID = SuperMarioBros-v0

#------
Epoch = 4
MiniBatch = 4
LearningRate = 0.0001


# PPO ->
PPOEps = 0.1
Entropy = 0.001

# ------ Exploration
# RND ->
NumEnv = 128
NumStep = 128
MaxStepPerEpisode = 18000
LifeDone = False
StateStackSize = 4
StickyAction = True
ActionProb = 0.25
IntGamma = 0.99
Gamma = 0.999
ExtCoef = 2
IntCoef = 1
UpdateProportion = 0.25
UseGAE = True
GAELambda = 0.95
PreProcHeight = 84
ProProcWidth = 84
ObsNormStep = 50
UseNoisyNet = False

# CNN Actor-Critic dims (from RND): refer to https://github.com/openai/random-network-distillation/blob/master/policies/cnn_policy_param_matched.py


# ------ Representation Learning

apply_same_transform_to_batch = True

# BYOL->
BYOL_projectionHiddenSize = 896
BYOL_projectionSize = 256
BYOL_movingAverageDecay = 0.99
BYOL_representationLossCoef = 1.0

# Barlow-Twins ->
BarlowTwinsLambda = 0.0051
BarlowTwinsProjectionSizes = [1024, 1024, 1024]
BarlowTwins_representationLossCoef = 1.0

# ------



loadModel = False
render = False
saveCkptEvery = 100
StableEps = 1e-8
UseGPU = True
UseGradClipping = False
MaxGradNorm = 0.5


[OPTIONS]
EnvType = [atari, mario, classic_control]




# ---------------------------------------------- Reference Values:
# [DEFAULT]
# TrainMethod = RND
# representationLearningMethod = BYOL

## EnvType = mario
## EnvID = SuperMarioBros-v0
## MaxStepPerEpisode = 18000
## ExtCoef = 5.

# EnvType = atari
# EnvID = MontezumaRevengeNoFrameskip-v4

# ------
# Epoch = 4 # number of optimization epochs
# MiniBatch = 4 # number of minibatches
# LearningRate = 0.0001


# # PPO ->
# PPOEps = 0.1 # PPO clip is calculated as surr2 = clamp(ratio, 1 - PPOEps, 1 + PPOEps)
# Entropy = 0.001 # entropy coefficient

# # RND ->
# NumEnv = 128 # number of parallel environments
# NumStep = 128
# MaxStepPerEpisode = 18000
# LifeDone = False
# StateStackSize = 4
# StickyAction = True
# ActionProb = 0.25 # sticky action probability
# IntGamma = 0.99 # gamma used for calculating the Return for intrinsic rewards (i.e. R_i = sum_over_t((intrinsic_gamma ** t) * intrinsic_reward_t)) (i.e. future reward discount factor)
# Gamma = 0.999 # gamma used for calculating the Return for extrinsic rewards (i.e. R_e = sum_over_t((intrinsic_gamma ** t) * extrinsic_reward_t) (i.e. future reward discount factor)
# ExtCoef = 2 # coefficient of extrinsic reward in the calculation of Combined Advantage (i.e. A = (A_i * IntCoef) + (A_e * ExtCoef)
# IntCoef = 1 # coefficient of intrinsic reward in the calculation of Combined Advantage (i.e. A = (A_i * IntCoef) + (A_e * ExtCoef)
# UpdateProportion = 0.25 # proportion of experience used for training predictor
# UseGAE = True
# GAELambda = 0.95 ; lambda iN GAE
# PreProcHeight = 84 # Height of image after preprocessing the state
# ProProcWidth = 84 # Width of image after preprocessing the state
# ObsNormStep = 50 # (numStep * ObsNormStep) number of initial steps are taken for initializing observation normalization
# UseNoisyNet = False

# # CNN Actor-Critic dims (from RND): refer to https://github.com/openai/random-network-distillation/blob/master/policies/cnn_policy_param_matched.py


# ------ Representation Learning

# apply_same_transform_to_batch = True # if False, then a new transformation (used for augmenting stacked states) is sampled per each element in the batch, otherwise (True) only one transformation is sampled per batch.

# # BYOL->
# BYOL_projectionHiddenSize = 896 # original on ImageNet is 4096
# BYOL_projectionSize = 256 # original on ImageNet is 256
# BYOL_movingAverageDecay = 0.99 # original on ImageNet is dynamically changing
# BYOL_representationLossCoef = 1.0 # BYOL loss is multiplied with this coefficient

# # Barlow-Twins ->
# BarlowTwinsLambda = 0.0051 # trade-off parameter lambda of the loss function
# BarlowTwinsProjectionSizes = [1024, 1024, 1024] # original on ImageNet is [8192, 8192, 8192]
# BarlowTwins_representationLossCoef = 1.0 # BarlowTwins loss is multiplied with this coefficient

# # ------



# loadModel = False
# render = False
# saveCkptEvery = 100 # after every this many episodes during training a checkpoint is saved
# StableEps = 1e-8
# # UseGPU = True
# UseGPU = False
# UseNorm = False
# ClipGradNorm = 0.5


# [OPTIONS]
# EnvType = [atari, mario]
